{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T_YHJjnD_Tja"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UrxyEKGn_ez7"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('50_Startups.csv')\n",
    "dataset.drop(\"State\", axis=\"columns\", inplace=True)\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 857
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 552,
     "status": "ok",
     "timestamp": 1586353652778,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "GOB3QhV9B5kD",
    "outputId": "4a05377a-2db2-43fc-b824-a0710448baee"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R&amp;D Spend</th>\n",
       "      <th>Administration</th>\n",
       "      <th>Marketing Spend</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165349.20</td>\n",
       "      <td>136897.80</td>\n",
       "      <td>471784.10</td>\n",
       "      <td>192261.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>162597.70</td>\n",
       "      <td>151377.59</td>\n",
       "      <td>443898.53</td>\n",
       "      <td>191792.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153441.51</td>\n",
       "      <td>101145.55</td>\n",
       "      <td>407934.54</td>\n",
       "      <td>191050.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144372.41</td>\n",
       "      <td>118671.85</td>\n",
       "      <td>383199.62</td>\n",
       "      <td>182901.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142107.34</td>\n",
       "      <td>91391.77</td>\n",
       "      <td>366168.42</td>\n",
       "      <td>166187.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   R&D Spend  Administration  Marketing Spend     Profit\n",
       "0  165349.20       136897.80        471784.10  192261.83\n",
       "1  162597.70       151377.59        443898.53  191792.06\n",
       "2  153441.51       101145.55        407934.54  191050.39\n",
       "3  144372.41       118671.85        383199.62  182901.99\n",
       "4  142107.34        91391.77        366168.42  166187.94"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kb_v_ae-A-20"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.35006454, -0.78547109,  0.1011968 ],\n",
       "       [-0.55530319, -1.48117426,  0.02734979],\n",
       "       [ 0.07935762,  0.80133381, -0.55152132],\n",
       "       [-0.54638238,  1.32505817,  0.07011684],\n",
       "       [ 0.43485371, -0.35598663,  0.75148516],\n",
       "       [ 1.26943143,  0.85518519,  0.98603118],\n",
       "       [ 1.04525007,  1.28077047,  0.4404    ],\n",
       "       [-1.529843  ,  0.02942065, -1.6218751 ],\n",
       "       [-1.53976251, -2.76767264, -1.6372965 ],\n",
       "       [-0.13115188,  1.14497701, -0.76949991],\n",
       "       [ 0.92791613, -0.02992062,  0.48303162],\n",
       "       [-0.20932933, -0.2993768 , -0.89915412],\n",
       "       [-0.17870828,  0.2251352 , -1.26401642],\n",
       "       [ 0.1374709 , -0.06929437,  0.50384666],\n",
       "       [-1.03967624, -1.05076697, -0.43852106],\n",
       "       [ 0.09938348, -0.36790317,  0.781818  ],\n",
       "       [-1.21580174,  0.15416247, -1.34947778],\n",
       "       [ 1.05822437,  0.97836757,  0.88670051],\n",
       "       [ 0.4401196 ,  0.46754749,  0.40923215],\n",
       "       [-0.15151937,  0.62430586, -0.51983056],\n",
       "       [ 1.30361149, -0.91073517,  1.30179825],\n",
       "       [ 0.49781135,  0.83770651,  0.65149135],\n",
       "       [-0.92897212, -0.18716957, -0.23769075],\n",
       "       [-1.55149779, -0.24751712, -1.27140496],\n",
       "       [ 1.96871085,  1.08106713,  1.95818096],\n",
       "       [ 0.48063418,  0.15177059,  0.38634632],\n",
       "       [-0.59739193, -2.78544219, -0.04140287],\n",
       "       [ 0.11649007, -0.93133851, -0.49867241],\n",
       "       [ 1.36290079,  0.91964899, -0.60281921],\n",
       "       [-0.08943162, -0.68142339,  0.83126112],\n",
       "       [-0.93093295,  0.14156607, -0.00821485],\n",
       "       [ 0.14561902,  1.1736151 ,  0.7905076 ],\n",
       "       [ 0.31947194,  1.16359793, -1.6372965 ],\n",
       "       [ 1.11867842, -0.56831342,  0.83298548],\n",
       "       [-0.71671353, -1.56095586, -0.21984184],\n",
       "       [-1.52301833, -0.29261949,  0.76926327],\n",
       "       [ 1.57413686, -0.18231009,  1.46653355],\n",
       "       [ 2.02828029,  0.52173299,  2.18404776],\n",
       "       [-1.55149779,  0.46491495, -1.6372965 ],\n",
       "       [-1.07135402,  1.21350725, -1.40779169]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(X_train)\n",
    "x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b, *argv):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      y : (ndarray Shape (m,))  target value \n",
    "      w : (ndarray Shape (n,))  values of parameters of the model      \n",
    "      b : (scalar)              value of bias parameter of the model\n",
    "      *argv : unused, for compatibility with regularized version below\n",
    "    Returns:\n",
    "      total_cost : (scalar) cost \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    total_cost = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        fwb=0\n",
    "        for j in range(n):\n",
    "            zwb = w[j]*X[i][j]\n",
    "            fwb = fwb + zwb \n",
    "        fwb = fwb + b\n",
    "#        fwb = 1/(1 + (np.exp(-zwb)))\n",
    " #       loss = (-y[i] * np.log(fwb)) - ((1 - y[i]) * np.log(1 - fwb))\n",
    "        cost = (fwb - y[i])**2\n",
    "        total_cost = total_cost + cost\n",
    "        \n",
    "        \n",
    "    total_cost = (1/m)*total_cost\n",
    "    ### END CODE HERE ### \n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at test w and b (non-zeros): 1279027177.021\n"
     ]
    }
   ],
   "source": [
    "# Compute and display cost with non-zero w and b\n",
    "test_w = np.array([0.2, 0.2, 0.2])\n",
    "test_b = -24.\n",
    "cost = compute_cost(X_train, y_train, test_w, test_b)\n",
    "\n",
    "print('Cost at test w and b (non-zeros): {:.3f}'.format(cost))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b, *argv): \n",
    "    \"\"\"\n",
    "    Computes the gradient for logistic regression \n",
    " \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      y : (ndarray Shape (m,))  target value \n",
    "      w : (ndarray Shape (n,))  values of parameters of the model      \n",
    "      b : (scalar)              value of bias parameter of the model\n",
    "      *argv : unused, for compatibility with regularized version below\n",
    "    Returns\n",
    "      dj_dw : (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db : (scalar)             The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros(w.shape)\n",
    "    dj_db = 0.\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    total_costw = 0\n",
    "    total_costb = 0\n",
    "    fwb = 0\n",
    "    for i in range(m):\n",
    "        # Calculate f_wb (exactly as you did in the compute_cost function above)\n",
    "        zwb=0\n",
    "#         lossw=0\n",
    "        \n",
    "        for j in range(n):\n",
    "            z = w[j]*X[i][j]\n",
    "            zwb = zwb + z \n",
    "        zwb = zwb + b\n",
    "        #f_wb = 1/(1 + (np.exp(-zwb)))\n",
    "\n",
    "       # Calculate the  gradient for b from this example\n",
    "        dj_db_i = zwb - y[i]# Your code here to calculate the error\n",
    "\n",
    "      # add that to dj_db\n",
    "        dj_db += dj_db_i\n",
    "\n",
    "      # get dj_dw for each attribute\n",
    "        for j in range(n):\n",
    "         # You code here to calculate the gradient from the i-th example for j-th attribute\n",
    "            dj_dw_ij = (zwb - y[i])* X[i][j]\n",
    "            dj_dw[j] += dj_dw_ij\n",
    "\n",
    "          # divide dj_db and dj_dw by total number of examples\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db at test w and b: -116405.27580000002\n",
      "dj_dw at test w and b: [-9112062015.550978, -14911629830.466908, -23540201863.09532]\n"
     ]
    }
   ],
   "source": [
    "# Compute and display cost and gradient with non-zero w and b\n",
    "test_w = np.array([ 0.2, -0.5, 0.2])\n",
    "test_b = -24\n",
    "dj_db, dj_dw  = compute_gradient(X_train, y_train, test_w, test_b)\n",
    "\n",
    "print('dj_db at test w and b:', dj_db)\n",
    "print('dj_dw at test w and b:', dj_dw.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X :    (ndarray Shape (m, n) data, m examples by n features\n",
    "      y :    (ndarray Shape (m,))  target value \n",
    "      w_in : (ndarray Shape (n,))  Initial values of parameters of the model\n",
    "      b_in : (scalar)              Initial value of parameter of the model\n",
    "      cost_function :              function to compute cost\n",
    "      gradient_function :          function to compute gradient\n",
    "      alpha : (float)              Learning rate\n",
    "      num_iters : (int)            number of iterations to run gradient descent\n",
    "      lambda_ : (scalar, float)    regularization constant\n",
    "      \n",
    "    Returns:\n",
    "      w : (ndarray Shape (n,)) Updated values of parameters of the model after\n",
    "          running gradient descent\n",
    "      b : (scalar)                Updated value of parameter of the model after\n",
    "          running gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of training examples\n",
    "    m = len(X)\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w_in = w_in - alpha * dj_dw               \n",
    "        b_in = b_in - alpha * dj_db              \n",
    "       \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            cost =  cost_function(X, y, w_in, b_in, lambda_)\n",
    "            J_history.append(cost)\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n",
    "            w_history.append(w_in)\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
    "        \n",
    "    return w_in, b_in, J_history, w_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0]\n",
      "Iteration    0: Cost 13325077378.94   \n",
      "Iteration 2000: Cost 81956029.11   \n",
      "Iteration 4000: Cost 81943543.65   \n",
      "Iteration 6000: Cost 81943542.34   \n",
      "Iteration 8000: Cost 81943542.34   \n",
      "Iteration 10000: Cost 81943542.34   \n",
      "Iteration 12000: Cost 81943542.34   \n",
      "Iteration 14000: Cost 81943542.34   \n",
      "Iteration 16000: Cost 81943542.34   \n",
      "Iteration 18000: Cost 81943542.34   \n",
      "Iteration 19999: Cost 81943542.34   \n",
      "the value of w:  [35974.50481155   760.88423203  4285.33658382]\n",
      "the value of b:  109446.44724999927\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "# initial_w = 0.01 * (np.random.rand(3) - 0.5)\n",
    "initial_w=np.array([0,0,0])\n",
    "initial_b = 0\n",
    "print(initial_w)\n",
    "# Some gradient descent settings\n",
    "iterations = 20000\n",
    "alpha = 0.01\n",
    "\n",
    "w,b, J_history,_ = gradient_descent(x_scaled ,y_train, initial_w, initial_b, \n",
    "                                   compute_cost, compute_gradient, alpha, iterations, 0)\n",
    "print(\"the value of w: \", w)\n",
    "print(\"the value of b: \", b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "no_iterations = np.arange(0,iterations)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(no_iterations, J_history)\n",
    "plt.xticks(np.arange(0,iterations,1000))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 757,
     "status": "ok",
     "timestamp": 1586353664008,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "ywPjx0L1BMiD",
    "outputId": "099836bc-4d85-4b4f-a488-093faf02e8cb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(x_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 951,
     "status": "ok",
     "timestamp": 1586353666678,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "TQKmwvtdBkyb",
    "outputId": "493436bf-a4ae-4374-ca16-0b0c25d19457"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.02e+09 1.03e+05]\n",
      " [4.76e+09 1.44e+05]\n",
      " [4.73e+09 1.46e+05]\n",
      " [1.77e+09 7.78e+04]\n",
      " [7.35e+09 1.91e+05]\n",
      " [4.20e+09 1.05e+05]\n",
      " [1.57e+09 8.12e+04]\n",
      " [2.69e+09 9.75e+04]\n",
      " [4.06e+09 1.10e+05]\n",
      " [6.75e+09 1.66e+05]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = regressor.predict(X_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35974.5    760.88  4285.34] 109446.44724999998\n"
     ]
    }
   ],
   "source": [
    "print(regressor.coef_,regressor.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPhYhte6t7H4wEK4xPpDWT7",
   "name": "Multiple Linear Regression",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
